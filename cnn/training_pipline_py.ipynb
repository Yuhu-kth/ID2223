{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuhu-kth/ID2223/blob/main/cnn/training_pipline_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8afbm3ChJHhY",
        "outputId": "10f51edb-6ada-4151-c436-66d5aa90fdea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPEHTK-nKIjq",
        "outputId": "0116a350-7276-4c33-f20b-e792989efbd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.8.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.11.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o7JZJP4rMZ4",
        "outputId": "1bbe0d68-8264-44c8-cd26-1551c607922c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\t\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/whisper/cnn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyi-W2HT16fO",
        "outputId": "9cc7660d-d4b4-436c-c8a7-f3a2b989a7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mcommon_voice\u001b[0m/         \u001b[01;34m__pycache__\u001b[0m/                training_config.json\n",
            " env.yml               README.md                   training_pipline.py\n",
            " feature_pipeline.py   token.txt                   utils.py\n",
            " __init__.py          'training_config (1).json'   \u001b[01;34mwhisper-small-hi\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0bNKP9UI3DI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import evaluate \n",
        "from datasets import load_dataset, DatasetDict, Audio\n",
        "import sys\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n74zkHo1msy"
      },
      "outputs": [],
      "source": [
        "def query_yes_no(question, default=\"yes\"):\n",
        "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
        "\n",
        "    \"question\" is a string that is presented to the user.\n",
        "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
        "            It must be \"yes\" (the default), \"no\" or None (meaning\n",
        "            an answer is required of the user).\n",
        "\n",
        "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
        "    \n",
        "    from: https://stackoverflow.com/questions/3041986/apt-command-line-interface-like-yes-no-input\n",
        "    \"\"\"\n",
        "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
        "    if default is None:\n",
        "        prompt = \" [y/n] \"\n",
        "    elif default == \"yes\":\n",
        "        prompt = \" [Y/n] \"\n",
        "    elif default == \"no\":\n",
        "        prompt = \" [y/N] \"\n",
        "    else:\n",
        "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
        "\n",
        "    while True:\n",
        "        sys.stdout.write(question + prompt)\n",
        "        choice = input().lower()\n",
        "        if default is not None and choice == \"\":\n",
        "            return valid[default]\n",
        "        elif choice in valid:\n",
        "            return valid[choice]\n",
        "        else:\n",
        "            sys.stdout.write(\"Please respond with 'yes' or 'no' \" \"(or 'y' or 'n').\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky7EJNck1aH9"
      },
      "outputs": [],
      "source": [
        "def load_common_voice(path=None, save_path=None):\n",
        "    \"\"\"function that loads or downloads and edits\n",
        "\n",
        "    Args:\n",
        "        path (string, optional): path to the dataset to load. Defaults to None. (directory)\n",
        "        save_path (string, optional): path where to save the loaded/downloaded dataset. Defaults to None. (directory)\n",
        "\n",
        "    Returns:\n",
        "        datasets.DatasetDict: common voice dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # if the save path already exists, ask the user whether they want to overwrite it\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        if not query_yes_no(f\"{save_path} already exists and will be overwritten. Continue?\"):\n",
        "            return\n",
        "        \n",
        "    # if the save path is same to load path (and they exist), we may want to load it instead\n",
        "    if path == save_path and path:\n",
        "        if query_yes_no(f\"{save_path} already exists. Do you want to load it instead?\"):\n",
        "            return DatasetDict.load_from_disk(save_path)\n",
        "    \n",
        "    print(\"Dataset loading started\")\n",
        "    \n",
        "    if path:\n",
        "        print(f\"Loading dataset from {path}...\")\n",
        "        return DatasetDict.load_from_disk(path)\n",
        "\n",
        "    print(\"Loading dataset from huggingface...\")\n",
        "    common_voice = DatasetDict()\n",
        "\n",
        "    common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"train+validation\", use_auth_token=True)\n",
        "    common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"test\", use_auth_token=True)\n",
        "    \n",
        "    print(\"Raw dataset loaded.\")\n",
        "\n",
        "    common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "    common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "    \n",
        "    feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")\n",
        "    \n",
        "    def prepare_dataset(batch):\n",
        "        # load and resample audio data from 48 to 16kHz\n",
        "        audio = batch[\"audio\"]\n",
        "\n",
        "        # compute log-Mel input features from input audio array \n",
        "        batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "        # encode target text to label ids \n",
        "        batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "        return batch\n",
        "\n",
        "    print(\"Mapping the dataset...\")\n",
        "    common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)\n",
        "    \n",
        "    print(\"Dataset ready for training.\")\n",
        "    \n",
        "    if SAVE_DATASET_PATH:\n",
        "        print(f\"Saving dataset to {save_path}...\")\n",
        "        common_voice.save_to_disk(save_path)\n",
        "        \n",
        "        return common_voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGxd2oebKHmJ"
      },
      "outputs": [],
      "source": [
        "LOAD_PRETRAINED = \"openai/whisper-small\"\n",
        "TRAINING_PARAMS = \"cpu\"\n",
        "LOAD_DATASET_PATH = \"common_voice\"\n",
        "SAVE_WEIGHTS = \"common_voice/whisper-small-weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSaQlEJxJAh-"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWEZDDfK2Gi1",
        "outputId": "93c46b2b-a9a1-437d-bb70-c85ccca054df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.8/dist-packages (2.5.1)\n",
            "Requirement already satisfied: levenshtein==0.20.2 in /usr/local/lib/python3.8/dist-packages (from jiwer) (0.20.2)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from levenshtein==0.20.2->jiwer) (2.13.7)\n"
          ]
        }
      ],
      "source": [
        "pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcI7y79psxGN"
      },
      "outputs": [],
      "source": [
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN4Zk86yszzv"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSbsXa2rs1-l"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_model(from_pretrained=\"openai/whisper-small\", save_path=SAVE_WEIGHTS):\n",
        "    \"\"\"function that returns the model to be trained on\n",
        "\n",
        "    Args:\n",
        "        from_pretrained (str, optional): pretrained weights to use. Defaults to \"openai/whisper-small\".\n",
        "        save_path (str, optional): path to save the weights so they don't need to be downloaded. If left none, they will not be saved. Defaults to None. \n",
        "\n",
        "    Returns:\n",
        "        transformers.WhisperForConditionalGeneration: huggingface transformer model\n",
        "    \"\"\"\n",
        "\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(from_pretrained)\n",
        "    model.config.forced_decoder_ids = None\n",
        "    model.config.suppress_tokens = []\n",
        "    \n",
        "    if not from_pretrained.split(\"/\")[0] == 'openai':\n",
        "        print(\"Weights loaded from local source.\")\n",
        "        return model\n",
        "    \n",
        "    if save_path:\n",
        "        print(f\"Saving downloaded weights to {save_path}...\")\n",
        "        model.save_pretrained(save_path)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdznrF46s4cb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_training_args():\n",
        "    \"\"\"loads the training config\n",
        "\n",
        "    Args:\n",
        "        params_key (str, optional): key in the json config of parameters to use. Defaults to 'training_config_05_12_22_v1'.\n",
        "\n",
        "    Returns:\n",
        "        transformers.Seq2SeqTrainingArguments: training arguments\n",
        "    \"\"\"\n",
        "\n",
        "    # import json\n",
        "\n",
        "    # with open(\"training_config.json\") as f:\n",
        "    #     training_params = json.load(f)\n",
        "    # print(training_params)\n",
        "    \n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        num_train_epochs=1,\n",
        "        output_dir=\"./whisper-small-hi\", \n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=1, \n",
        "        learning_rate=1e-5,\n",
        "        warmup_steps=2,\n",
        "        max_steps=2,\n",
        "        gradient_checkpointing= True,\n",
        "        fp16= False,\n",
        "        evaluation_strategy= \"steps\",\n",
        "        per_device_eval_batch_size= 4,\n",
        "        predict_with_generate= True,\n",
        "        generation_max_length= 225,\n",
        "        save_steps= 2,\n",
        "        eval_steps= 2, \n",
        "        logging_steps= 1,\n",
        "        report_to= [\"tensorboard\"],\n",
        "        load_best_model_at_end= True,\n",
        "        metric_for_best_model= \"wer\",\n",
        "        greater_is_better= False,\n",
        "        push_to_hub= True,\n",
        "    )\n",
        "    # training_params = {\n",
        "    #     \"num_train_epochs\": 2,\n",
        "    #     \"output_dir\": \"./whisper-small-hi\",\n",
        "    #     \"per_device_train_batch_size\": 16,\n",
        "    #     \"gradient_accumulation_steps\": 1,\n",
        "    #     \"learning_rate\": 1e-5,\n",
        "    #     \"warmup_steps\": 500,\n",
        "    #     \"max_steps\": 500,\n",
        "    #     \"gradient_checkpointing\": true,\n",
        "    #     \"fp16\": false,\n",
        "    #     \"evaluation_strategy\": \"steps\",\n",
        "    #     \"per_device_eval_batch_size\": 8,\n",
        "    #     \"predict_with_generate\": true,\n",
        "    #     \"generation_max_length\": 225,\n",
        "    #     \"save_steps\": 1000,\n",
        "    #     \"eval_steps\": 1000,\n",
        "    #     \"logging_steps\": 25,\n",
        "    #     \"report_to\": [\"tensorboard\"],\n",
        "    #     \"load_best_model_at_end\": true,\n",
        "    #     \"metric_for_best_model\": \"wer\",\n",
        "    #     \"greater_is_better\": false,\n",
        "    #     \"push_to_hub\": true\n",
        "    # },\n",
        "\n",
        "    return training_args  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7UJaJQOZ2Bd"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "NKGi4Ym3s79I",
        "outputId": "5bedf062-71cc-45c3-b1e3-377d47a0dca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started training pipeline.\n",
            "Loading model with pretrained openai/whisper-small...\n",
            "Saving downloaded weights to common_voice/whisper-small-weights...\n",
            "Model loaded.\n",
            "Loading training params from the config file, cpu...\n",
            "Training params loaded.\n",
            "Loading the common voice dataset from common_voice...\n",
            "Dataset loading started\n",
            "Loading dataset from common_voice...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Common voice loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/whisper/cnn/./whisper-small-hi is already a clone of https://huggingface.co/Hannnnnah/whisper-small-hi. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/gdrive/MyDrive/whisper/cnn/./whisper-small-hi is already a clone of https://huggingface.co/Hannnnnah/whisper-small-hi. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Feature extractor saved in ./whisper-small-hi/preprocessor_config.json\n",
            "tokenizer config file saved in ./whisper-small-hi/tokenizer_config.json\n",
            "Special tokens file saved in ./whisper-small-hi/special_tokens_map.json\n",
            "added tokens file saved in ./whisper-small-hi/added_tokens.json\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 12360\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2\n",
            "  Number of trainable parameters = 241734912\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 03:46, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='1268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  51/1268 50:02 < 20:17:58, 0.02 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5069\n",
            "  Batch size = 4\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Started training pipeline.\")\n",
        "\n",
        "    print(f\"Loading model with pretrained {LOAD_PRETRAINED}...\")\n",
        "    model = load_model(LOAD_PRETRAINED)\n",
        "    print(\"Model loaded.\")\n",
        "    \n",
        "    print(f\"Loading training params from the config file, {TRAINING_PARAMS}...\")\n",
        "    training_args = load_training_args()\n",
        "    # TRAINING_PARAMS = \"cpu\"\n",
        "    print(\"Training params loaded.\")\n",
        "\n",
        "    if not LOAD_DATASET_PATH:\n",
        "        print(\"Creating and loading the common voice dataset...\")\n",
        "    else:\n",
        "        print(f\"Loading the common voice dataset from {LOAD_DATASET_PATH}...\")\n",
        "    common_voice = load_common_voice(path=LOAD_DATASET_PATH)\n",
        "    print(\"Common voice loaded.\")\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        args=training_args,\n",
        "        model=model,\n",
        "        train_dataset=common_voice[\"train\"],\n",
        "        eval_dataset=common_voice[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=processor.feature_extractor,\n",
        "    )\n",
        "\n",
        "    processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "    print(\"Training starting...\")\n",
        "    trainer.train()\n",
        "\n",
        "    kwargs = {\n",
        "        \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "        \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "        \"dataset_args\": \"config: sv, split: test\",\n",
        "        \"language\": \"sv\",\n",
        "        \"model_name\": \"Whisper Small Sv - Swedish\",  # a 'pretty' name for our model\n",
        "        \"finetuned_from\": \"openai/whisper-small\",\n",
        "        \"tasks\": \"automatic-speech-recognition\",\n",
        "        \"tags\": \"hf-asr-leaderboard\",\n",
        "    }\n",
        "\n",
        "    print(\"Pushing the model...\")\n",
        "    trainer.push_to_hub(**kwargs)\n",
        "    print(\"Training pipeline finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi8E8TXVfPvN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv35ZZV9lUIu2moK7SEYDX",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}